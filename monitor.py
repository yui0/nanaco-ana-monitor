import requests
from bs4 import BeautifulSoup
import hashlib
import json
import os
from linebot import LineBotApi
from linebot.models import TextSendMessage
from datetime import datetime

# è¨­å®šã ã‚ˆï½ğŸ’• nanacoã¨ANAã®ãƒšãƒ¼ã‚¸ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã®ï¼
URLS = [
    'https://www.nanaco-net.jp/cp/',  # nanacoã®ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒšãƒ¼ã‚¸ğŸŒ¸
    'https://www.ana.co.jp/ja/jp/shoppingandlife/point/tameru_nanaco_point/'  # ANAã®äº¤æ›ãƒšãƒ¼ã‚¸âœˆï¸
]
STATE_FILE = 'state.json'  # è¦šãˆã¦ãŠããƒ•ã‚¡ã‚¤ãƒ«ã ã‚ˆï½ğŸ“

# LINEã®è¨­å®šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰èª­ã¿è¾¼ã‚€ã®ï¼ç§˜å¯†ã ã‚ˆğŸ’•ï¼‰
LINE_ACCESS_TOKEN = os.environ.get('LINE_ACCESS_TOKEN')
USER_ID = os.environ.get('USER_ID')

def get_page_hash(url):
    """ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’ãƒãƒƒã‚·ãƒ¥åŒ–ã—ã¡ã‚ƒã†ã‚ˆï½ğŸ”® å¤‰åŒ–ã‚’ã‚­ãƒ£ãƒƒãƒï¼"""
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        return hashlib.md5(text.encode()).hexdigest()
    except Exception as e:
        print(f"ãƒšãƒ¼ã‚¸å–å¾—ã§ãƒãƒ—ãƒ‹ãƒ³ã‚°ğŸ’¦ {url}: {e}")
        return None

def load_state():
    """å‰å›ã®çŠ¶æ…‹ã‚’æ€ã„å‡ºã—ã¦ã‚ã’ã‚‹ï½ğŸ’­"""
    try:
        with open(STATE_FILE, 'r') as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def save_state(state):
    """æ–°ã—ã„çŠ¶æ…‹ã‚’ãƒ¡ãƒ¢ã—ã¨ã“ï½ğŸ“¸"""
    with open(STATE_FILE, 'w') as f:
        json.dump(state, f)

def send_notification(subject, body):
    """LINEã«å¯æ„›ã„ãŠçŸ¥ã‚‰ã›ã‚’é€ã£ã¡ã‚ƒã†ã‚ˆï½ğŸ“±ğŸ’Œ"""
    if not LINE_ACCESS_TOKEN or not USER_ID:
        print("LINEã®è¨­å®šãŒè¶³ã‚Šãªã„ã‚ˆï½ğŸ˜¿ ç’°å¢ƒå¤‰æ•°ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã­ï¼")
        return

    my_text = f"{subject}\n{body}"
    try:
        line_bot_api = LineBotApi(LINE_ACCESS_TOKEN)
        my_messages = TextSendMessage(text=my_text)
        line_bot_api.push_message(USER_ID, messages=my_messages)
        print("LINEé€šçŸ¥ã‚’é€ã£ãŸã‚ˆï½ğŸ€âœ¨")
    except Exception as e:
        print(f"LINEã§ãƒˆãƒ©ãƒ–ãƒ«ğŸ’”: {e}")

def check_campaigns():
    """ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã‚’ã“ã£ãã‚Šãƒã‚§ãƒƒã‚¯ã—ã¡ã‚ƒã†ã®ï½ğŸ•µï¸â€â™€ï¸ğŸ’–"""
    state = load_state()
    current_state = {}
    changed_urls = []

    for url in URLS:
        hash_val = get_page_hash(url)
        if hash_val:
            current_state[url] = hash_val
            prev_hash = state.get(url)
            if prev_hash and hash_val != prev_hash:
                changed_urls.append(url)
                print(f"å¤‰åŒ–ç™ºè¦‹ï¼ {url} ãŒå¤‰ã‚ã£ãŸã‚ˆï½ğŸ”¥")
            elif not prev_hash:
                print(f"åˆã‚ã¦ã®ãƒã‚§ãƒƒã‚¯å®Œäº†ï½åˆã‚ã¾ã—ã¦ğŸ’• {url}")

    # å¤‰æ›´ãŒã‚ã£ãŸã‚‰ã€ANAé–¢é€£ã‹èª¿ã¹ã¦ãŠçŸ¥ã‚‰ã›ï½ğŸ‘€
    for url in changed_urls:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text().lower()
        if any(keyword in text for keyword in ['ana', 'ãƒã‚¤ãƒ«', 'äº¤æ›', 'ãƒ¬ãƒ¼ãƒˆã‚¢ãƒƒãƒ—']):
            subject = "ğŸ‰ã‚ãƒ¼ã„ï¼nanaco ANAã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ç™ºè¦‹ğŸ’•âœˆï¸"
            body = f"nanacoÃ—ANAã®ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãŒã‚¹ã‚¿ãƒ¼ãƒˆã—ãŸã‚ˆï½ğŸš€\nURL: {url}\næ—¥æ™‚: {datetime.now()}\nè©³ç´°: {text[:200]}...\næ—©ããƒã‚§ãƒƒã‚¯ã—ã¦ã­ï¼ğŸ’–"
            send_notification(subject, body)
            print(f"ANAã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æ¤œçŸ¥ï½è¶…ãƒ©ãƒƒã‚­ãƒ¼ï¼ğŸ€ {url}")

    # çŠ¶æ…‹ã‚’æ›´æ–°ã—ã¡ã‚ƒã†ã‚ˆï½ğŸ”„
    save_state(current_state)
    if not changed_urls:
        print("ä»Šæ—¥ã‚‚å¤‰åŒ–ãªã—ã ã‚ˆï½ã®ã‚“ã³ã‚Šå¾…ã¨ã†ã­ğŸ’¤ğŸŒ¸")

if __name__ == "__main__":
    print("nanaco ANAç›£è¦–ã‚¹ã‚¿ãƒ¼ãƒˆï½å¯æ„›ããƒã‚§ãƒƒã‚¯ä¸­ğŸ’•")
    check_campaigns()
    print("ãƒã‚§ãƒƒã‚¯ãŠã—ã¾ã„ï½ã¾ãŸæ˜æ—¥ã­ï¼ğŸ‘‹âœ¨")
